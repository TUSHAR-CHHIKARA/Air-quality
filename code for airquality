# Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from statsmodels.graphics.tsaplots import plot_acf

# Load the dataset
file_path = "/content/AQI_Bulletin_20250224.xlsx"
df = pd.read_excel(file_path)

# ðŸ”¹ Step 1: Data Cleaning
df.dropna(inplace=True)  # Remove missing values
df['Index Value'] = pd.to_numeric(df['Index Value'], errors='coerce')  # Convert AQI to numeric
df[['Stations Participated', 'Total Stations']] = df['No. of Stations Participated/ Total Stations'].str.split('/', expand=True)
df['Stations Participated'] = pd.to_numeric(df['Stations Participated'], errors='coerce')
df['Total Stations'] = pd.to_numeric(df['Total Stations'], errors='coerce')
df.drop(columns=['No. of Stations Participated/ Total Stations'], inplace=True)  # Drop redundant column

# ðŸ”¹ Step 2: Feature Scaling
scaler = MinMaxScaler()
df[['Index Value', 'Stations Participated', 'Total Stations']] = scaler.fit_transform(df[['Index Value', 'Stations Participated', 'Total Stations']])

# ðŸ”¹ Step 3: Prepare Data for LSTM (Time-Series Sequences)
def create_sequences(data, target_col, sequence_length=20):
    sequences, labels = [], []
    for i in range(len(data) - sequence_length):
        sequences.append(data.iloc[i:i + sequence_length][['Stations Participated', 'Total Stations']].values)
        labels.append(data.iloc[i + sequence_length][target_col])
    return np.array(sequences), np.array(labels)

sequence_length = 20  # Increase sequence length for better learning
X, y = create_sequences(df, 'Index Value', sequence_length)

# Split into Train & Test Sets (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Reshape Input for LSTM
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 2))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 2))

# ðŸ”¹ Step 4: Build the Improved LSTM Model
model = Sequential([
    LSTM(100, return_sequences=True, input_shape=(sequence_length, 2)),  # Increased units
    Dropout(0.2),
    LSTM(50, return_sequences=True),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25, activation='relu'),
    Dense(1)  # Output layer for regression
])

# Compile the Model with Lower Learning Rate
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')

# ðŸ”¹ Step 5: Train the Model
model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))

# ðŸ”¹ Step 6: Evaluate the Model
y_pred = model.predict(X_test).flatten()

# Compute Metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print Results
print(f"ðŸ”¹ Mean Squared Error (MSE): {mse:.4f}")
print(f"ðŸ”¹ Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"ðŸ”¹ Mean Absolute Error (MAE): {mae:.4f}")
print(f"ðŸ”¹ RÂ² Score: {r2:.4f}")

# ðŸ”¹ Step 7: Visualization
# ðŸ“Œ Line Plot - Actual vs Predicted AQI
plt.figure(figsize=(10,5))
plt.plot(y_test, label="Actual AQI", color='blue')
plt.plot(y_pred, label="Predicted AQI", color='red', linestyle="dashed")
plt.xlabel("Time")
plt.ylabel("AQI")
plt.title("Actual vs Predicted AQI")
plt.legend()
plt.show()

# ðŸ“Œ Scatter Plot - Actual vs Predicted AQI
plt.figure(figsize=(6,6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel("Actual AQI")
plt.ylabel("Predicted AQI")
plt.title("Actual vs Predicted AQI (Scatter Plot)")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color="red")
plt.show()

# ðŸ“Œ Residual Analysis - Error Distribution
residuals = y_test - y_pred
plt.figure(figsize=(10,5))
plt.plot(residuals, label="Residuals", color='purple')
plt.axhline(y=0, color='red', linestyle="dashed")
plt.xlabel("Time")
plt.ylabel("Residual (Error)")
plt.title("Residual Plot")
plt.legend()
plt.show()

# ðŸ“Œ Histogram of Residuals
plt.figure(figsize=(8,5))
sns.histplot(residuals, kde=True, color='purple', bins=20)
plt.xlabel("Residual Error")
plt.ylabel("Frequency")
plt.title("Residual Error Distribution")
plt.show()

# ðŸ“Œ Autocorrelation of Residuals
plot_acf(residuals)
plt.title("Autocorrelation of Residuals")
plt.show()

# ðŸ“Œ Train vs Test Loss
train_loss = model.evaluate(X_train, y_train, verbose=0)
test_loss = model.evaluate(X_test, y_test, verbose=0)
print(f"ðŸ”¹ Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}")

# ðŸ“Œ Cross-Validation
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True)
fold = 1
for train_idx, val_idx in kf.split(X):
    X_train_fold, X_val_fold = X[train_idx], X[val_idx]
    y_train_fold, y_val_fold = y[train_idx], y[val_idx]

    model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=16, verbose=0)
    val_loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)
    print(f"Fold {fold} Validation Loss: {val_loss:.4f}")
    fold += 1
